name: Vertex MLOps Pipeline

on:
  push:
    branches: ["main"]
  workflow_dispatch: {}

jobs:
  vertex-mlops:
    runs-on: ubuntu-latest

    steps:
    # 1) Checkout du repo
    - name: Checkout
      uses: actions/checkout@v4

    # 2) Auth Google Cloud
    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    # 3) Setup gcloud
    - name: Setup gcloud
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}

    # 4) Build package Python (tar.gz)
    - name: Build Python package for Vertex
      run: tar -czf src.tar.gz src setup.py

    # 5) Upload vers GCS
    - name: Upload package to GCS
      run: gsutil cp src.tar.gz gs://${{ secrets.GCS_BUCKET }}/code/src.tar.gz

    # 6) Install deps localement
    - name: Install dependencies
      run: pip install -r requirements.txt

    # 7) Fetch dataset + upload
    - name: Fetch & prepare dataset
      run: |
        RUN_ID=$(date +%s)
        echo "$RUN_ID" > run_id.txt
        python3 src/fetch_and_train.py --run_id $RUN_ID --pair BTC/USDT

    # 8) Création du job Vertex + récupération job_id
    - name: Launch Vertex AI Job
      run: |
        RUN_ID=$(cat run_id.txt)
        echo "RUN_ID=$RUN_ID"

        JOB_ID=$(gcloud ai custom-jobs create \
          --region=${{ secrets.VERTEX_REGION }} \
          --display-name="train-${RUN_ID}" \
          --python-package-uris=gs://${{ secrets.GCS_BUCKET }}/code/src.tar.gz \
          --worker-pool-spec=machine-type=e2-standard-4,executor-image-uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest,python-module=MLOPS_Crypto.train_lstm \
          --args=--data_prefix=gs://${{ secrets.GCS_BUCKET }}/ohlcv-data \
          --args=--pair=BTC_USDT \
          --args=--run_id=$RUN_ID \
          --format="value(name)" )

        echo "$JOB_ID" > job_id.txt
        echo "Launched job: $JOB_ID"

    # 9) Attente FIXE 18 minutes
    - name: Wait for Vertex AI Training Job
      run: |
        echo "Waiting 18 minutes to let Vertex finish training..."
        for i in {1..18}; do
          echo "Minute $i/18..."
          sleep 60
        done
        echo "Done waiting. Continuing pipeline."

    # 10) Install deps local pour évaluation
    - name: Install local dependencies (evaluate)
      run: pip install -r requirements.txt

    # 10.5) Préparer dataset local pour evaluate.py
    - name: Prepare dataset for evaluate.py
      run: |
        RUN_ID=$(cat run_id.txt)
        mkdir -p data/prepared
        python3 src/prepare.py data/BTC_USDT_${RUN_ID}_raw.csv data/prepared

    # 11) Évaluation du modèle (plots hyperparams, courbes loss, etc.)
    - name: Evaluate trained model
      run: |
        RUN_ID=$(cat run_id.txt)
        MODEL_DIR="local_model_${RUN_ID}"
        mkdir -p $MODEL_DIR

        # Recherche dynamique du dossier modèle
        MODEL_PATH=$(gsutil ls gs://${{ secrets.GCS_BUCKET }}/ohlcv-data/models/ | grep "BTC_USDT_${RUN_ID}_")
        if [ -z "$MODEL_PATH" ]; then
          echo "Model folder not found in GCS!"
          exit 1
        fi

        echo "Found model folder: $MODEL_PATH"

        # Téléchargement des artefacts entraînés
        gsutil cp -r ${MODEL_PATH}/* $MODEL_DIR/

        # Lancement de l’évaluation
        python3 src/evaluate.py $MODEL_DIR data/prepared

    # 12) Visualisation du modèle
    - name: Visualize trained model
      run: |
        RUN_ID=$(cat run_id.txt)
        python3 src/visualize_model.py \
          --bucket ${{ secrets.GCS_BUCKET }} \
          --gcs_path ohlcv-data \
          --lookback 30 \
          --run_id $RUN_ID

    # 13) Upload graphes générés (evaluate + visualize)
    - name: Upload evaluation plots
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-plots
        path: evaluation/
